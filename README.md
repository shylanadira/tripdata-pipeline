# Capstone Project 3: Batch & Streaming Taxi Data Pipeline

![Project Architecture Diagram](https://github.com/user-attachments/assets/c0b2c0ba-9480-4fe4-a90e-3cb07ea16821)


---

## ğŸ“š Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Data Ingestion](#data-ingestion)
- [Data Transformation](#data-transformation)
- [Virtual Environment](#virtual-environment)
- [Docker & Aiflow](#docker)
- [Folder Structure](#folder-structure)
- [Logging and Monitoring](#logging-and-monitoring)

---

## ğŸ” Overview

This project builds an end-to-end **batch and streaming data pipeline** to ingest, transform, and load NYC taxi trip data into BigQuery. It integrates:

- **Python scripts** for extraction, transformation, and loading (ETL)
- **Pub/Sub + Dataflow** for streaming ingestion
- **DBT** for transformation in BigQuery
- **Airflow** for scheduling inside **Docker**

The result is a scalable and automated pipeline with both historical and real-time data support.

---

## âš™ï¸ Architecture

- **Batch Flow:**  
  CSV + JSON â†’ GCS â†’ BigQuery (`staging_trip_data`) â†’ DBT â†’ `final_trip_data`

- **Streaming Flow:**  
  Python `publisher.py` â†’ Pub/Sub â†’ Dataflow â†’ BigQuery (`trip_data_stream`) â†’ DBT merge

- **Airflow:** schedules all batch tasks daily in Docker

---

## ğŸ“¥ Data Ingestion

### Batch (Extractor & Loader):

1. **Raw files:**  
   - `data/csv/green_tripdata_0.csv` to `green_tripdata_35.csv`  
   - `data/json/green_tripdata_36.json` to `green_tripdata_52.json`  
   - Lookup: `payment_type.csv`, `time_zone.csv`

2. **Scripts:**
   - `merge_data.py`: Merges all files into `green_tripdata_full.csv`
   - `extractor.py`: Uploads CSVs to GCS
   - `loader.py`: Loads data into `staging_trip_data` in BigQuery

3. **Tools:**  
   - Python `google-cloud-storage`, `google-cloud-bigquery`

---

### Streaming:

1. **Dummy Generator:** `publisher.py`
   - Sends JSON taxi records to Pub/Sub topic `capstone3_streaming_shyla`

2. **Stream Processor:** `beam_pipeline.py`
   - Reads from Pub/Sub â†’ transforms â†’ loads into `trip_data_stream` (BQ)

3. **Schema:** Matches batch schema (with pickup, dropoff, fare, etc.)

---

## ğŸ§ª Data Transformation

Using **DBT**, transformation happens in 2 layers:

### 1. `stg_full_trip_data.sql`  
Appends batch + streaming data into one staging table (incremental)

### 2. `final_trip_data.sql`

Transforms data by:

- âœ… Calculating `trip_duration`  
- âœ… Converting column names to `snake_case`  
- âœ… Joining `payment_type` table for readable labels  
- âœ… Converting `trip_distance` from miles to kilometers  
- âœ… Materialized as an `incremental` table

---

## ğŸ§ª Virtual Environment

```bash
python -m venv venv
.\venv\Scripts\activate
pip install dbt-bigquery apache-beam google-cloud-storage google-cloud-bigquery
```
## Docker & Airflow
### `docker-compose.yaml`

Running Airflow:
```bash
docker-compose up --build
```
Access Airflow at: http://localhost:8080
Login with: admin / admin

DAG:
### `dags/dbt_dag.py`

## Folder Structure
```pgsql
capstone3/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ csv/
â”‚   â”œâ”€â”€ json/
â”‚   â”œâ”€â”€ payment_type.csv
â”‚   â”œâ”€â”€ time_zone.csv
â”‚   â””â”€â”€ green_tripdata_full.csv
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ dbt_dag.py
â”œâ”€â”€ batch_pipeline/
â”‚   â”œâ”€â”€ merge_data.py
â”‚   â”œâ”€â”€ extractor.py
â”‚   â””â”€â”€ loader.py
â”œâ”€â”€ streaming_pipeline/
â”‚   â”œâ”€â”€ publisher.py
â”‚   â””â”€â”€ beam_pipeline.py
â”œâ”€â”€ dbt_capstone/
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ staging/
â”‚       â”‚   â”œâ”€â”€ stg_trip_data.sql
â”‚       â”‚   â”œâ”€â”€ stg_full_trip_data.sql
â”‚       â”‚   â””â”€â”€ stg_trip_stream.sql
â”‚       â””â”€â”€ marts/
â”‚           â””â”€â”€ final_trip_data.sql
â”œâ”€â”€ secrets/
â”‚   â””â”€â”€ key.json
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```
## ğŸ“Š Logging and Monitoring

- All Python scripts (`transformer.py`, `extractor.py`, `loader.py`) use the built-in `logging` module.
- Logs track progress during merging, uploading, and transforming processes.
- Airflow UI provides detailed visualization and logs for each task.
- Google Cloud Console enables monitoring for Pub/Sub and Dataflow (streaming) components.



